{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as kr\n",
    "\n",
    "import data\n",
    "import net\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed data\n",
      "Data Loaded\n"
     ]
    }
   ],
   "source": [
    "# Force new Pre-processing\n",
    "pre_process = False\n",
    "if not pre_process: # First try to load old data\n",
    "    if data.check_load():\n",
    "        print(\"Loading pre-processed data\")\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = data.load_pre_processed()\n",
    "    else:\n",
    "        pre_process = True\n",
    "if pre_process: # Load raw data\n",
    "    print(\"Loading raw Data\")\n",
    "    X = []\n",
    "    y = []\n",
    "    Xt = []\n",
    "    yt = []\n",
    "\n",
    "    s0 = []\n",
    "    sv0 = []\n",
    "\n",
    "    n_s = 22\n",
    "    n_test = 50\n",
    "\n",
    "    for i in range(1, 10):\n",
    "        xi, yi = data.load(i)\n",
    "        yi = data.recode_y(yi)\n",
    "\n",
    "        xi, yi = data.drop_nan(xi, yi)\n",
    "\n",
    "        xi, yi = data.shuffle(xi[:, :n_s], yi)\n",
    "\n",
    "        yi = kr.utils.to_categorical(yi)\n",
    "\n",
    "        Xt.append(xi[:n_test])\n",
    "        yt.append(yi[:n_test])\n",
    "\n",
    "        X.append(xi[n_test:])\n",
    "        y.append(yi[n_test:])\n",
    "        print(\"Shape %d:\" % i, X[-1].shape)\n",
    "\n",
    "print(\"Data Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if pre_process:\n",
    "    mean = []\n",
    "    var = []\n",
    "    X_train = None\n",
    "    y_train = None\n",
    "\n",
    "    X_test = None\n",
    "    y_test = None\n",
    "\n",
    "    X_val = None\n",
    "    y_val = None\n",
    "\n",
    "    X_train = np.concatenate(X, axis=0)\n",
    "    X_test = np.concatenate(Xt, axis=0)\n",
    "\n",
    "    y_train = np.concatenate(y, axis=0)\n",
    "    y_test = np.concatenate(yt, axis=0)\n",
    "    print(\"Data merged\")\n",
    "\n",
    "    # Butterworth Filter\n",
    "    butt = 1\n",
    "    if butt:\n",
    "        X_train = data.butter_band(X_train, 4, 120)\n",
    "        X_test = data.butter_band(X_test, 4, 120)\n",
    "        print(\"Butter filter applied\")\n",
    "\n",
    "    \n",
    "    # Normalize Data (uses Keepdims)\n",
    "    ax = (0, 2)\n",
    "    X_train, m, v = data.normalize_data(X_train, None, None, axis=ax)\n",
    "    X_test, _, _ = data.normalize_data(X_test, m, v)#, None, None, axis=ax)\n",
    "\n",
    "\n",
    "    # Make validation set\n",
    "    X_train, y_train = data.shuffle(X_train, y_train)\n",
    "    val_split = 0.2\n",
    "    num_val = int(val_split*(y_train.shape[0]))\n",
    "    X_val = X_train[:num_val]\n",
    "    X_train = X_train[num_val:]\n",
    "    y_val = y_train[:num_val]\n",
    "    y_train = y_train[num_val:]\n",
    "    print(\"Data standardized\")\n",
    "\n",
    "    # Window the data\n",
    "    window = 500\n",
    "    stride = 50\n",
    "    start = 0\n",
    "\n",
    "    X_train, y_train = data.windowing(X_train, y_train, start, window, stride=stride)\n",
    "    X_test, y_test = data.windowing(X_test, y_test, start, window, stride=stride)\n",
    "    # Validation cut\n",
    "    X_val, y_val = data.windowing(X_val, y_val, start, window, stride=stride)\n",
    "    print(\"Windowing applied: w {}; s {}\".format(window, stride))\n",
    "\n",
    "    \n",
    "    # Noise augmentation (not used(?))\n",
    "    noise = 0\n",
    "    if noise > 0:\n",
    "        X_train, y_train = data.augment_noise(X_train, y_train, p=noise)\n",
    "        print(\"Noise augmented\")\n",
    "\n",
    "    \n",
    "    # Swap axes to match RNN\n",
    "    X_train = data.swap_axis(X_train)\n",
    "    X_test = data.swap_axis(X_test)\n",
    "    X_val = data.swap_axis(X_val)\n",
    "    \n",
    "    data.save_pre_processed(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    print(\"Preprocessing complete. Train shape: {}\".format(X_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Three models are tested:\n",
    "#### 1. Stride of 2 on 1D convolutions\n",
    "#### 2. Stride of 1 and use max pools\n",
    "#### 3. Stride of 1 and use average pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-76f46808a492>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m RNNconfig = {\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m'num_steps'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m'sensors'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'state_size'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "RNNconfig = {\n",
    "    'num_steps' : X_train.shape[1],\n",
    "    'sensors' : X_train.shape[2],\n",
    "    # LSTM\n",
    "    'state_size' : 32,\n",
    "\n",
    "    # CNN\n",
    "    'filters' : 32,\n",
    "    'strides' : 2,\n",
    "    # Output\n",
    "    'output_size' : 4,\n",
    "    \n",
    "    # Activations\n",
    "    'c_act' : 'relu',\n",
    "    'r_act' : 'hard_sigmoid',\n",
    "    'rk_act' : 'tanh',\n",
    "    \n",
    "    'batch_size' : 512,\n",
    "    'learning_rate' : 0.0012,\n",
    "    'epochs' : 200,\n",
    "    'reg' : 0.001,\n",
    "    \n",
    "    'rec_drop': 0.32,\n",
    "    'drop' : 0.5,\n",
    "    'cnn_drop' : 0.6,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1: Run RNN with stride of 2 on convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1 initialized.\n",
      "Training model_1\n",
      "Train on 18557 samples, validate on 4631 samples\n",
      "Epoch 1/200\n",
      "18557/18557 [==============================] - 31s 2ms/step - loss: 1.7482 - acc: 0.2572 - val_loss: 1.6394 - val_acc: 0.2485\n",
      "Epoch 2/200\n",
      "18557/18557 [==============================] - 25s 1ms/step - loss: 1.5869 - acc: 0.2626 - val_loss: 1.5390 - val_acc: 0.2915\n",
      "Epoch 3/200\n",
      "18557/18557 [==============================] - 25s 1ms/step - loss: 1.5176 - acc: 0.2601 - val_loss: 1.5029 - val_acc: 0.2237\n",
      "Epoch 4/200\n",
      "18557/18557 [==============================] - 25s 1ms/step - loss: 1.4809 - acc: 0.2701 - val_loss: 1.4762 - val_acc: 0.2272\n",
      "Epoch 5/200\n",
      "18557/18557 [==============================] - 25s 1ms/step - loss: 1.4580 - acc: 0.2692 - val_loss: 1.4533 - val_acc: 0.2481\n",
      "Epoch 6/200\n",
      "18557/18557 [==============================] - 25s 1ms/step - loss: 1.4393 - acc: 0.2778 - val_loss: 1.4474 - val_acc: 0.2349\n",
      "Epoch 7/200\n",
      "18557/18557 [==============================] - 25s 1ms/step - loss: 1.4247 - acc: 0.2830 - val_loss: 1.4300 - val_acc: 0.2377\n",
      "Epoch 8/200\n",
      "18557/18557 [==============================] - 25s 1ms/step - loss: 1.4124 - acc: 0.2887 - val_loss: 1.4200 - val_acc: 0.2442\n",
      "Epoch 9/200\n",
      "18557/18557 [==============================] - 25s 1ms/step - loss: 1.4017 - acc: 0.3012 - val_loss: 1.4196 - val_acc: 0.2522\n",
      "Epoch 10/200\n",
      "18557/18557 [==============================] - 25s 1ms/step - loss: 1.3937 - acc: 0.3038 - val_loss: 1.3962 - val_acc: 0.3166\n",
      "Epoch 11/200\n",
      "18557/18557 [==============================] - 25s 1ms/step - loss: 1.3881 - acc: 0.3030 - val_loss: 1.3932 - val_acc: 0.2716\n",
      "Epoch 12/200\n",
      "18557/18557 [==============================] - 25s 1ms/step - loss: 1.3842 - acc: 0.3120 - val_loss: 1.3873 - val_acc: 0.2732\n",
      "Epoch 13/200\n",
      "18557/18557 [==============================] - 25s 1ms/step - loss: 1.3787 - acc: 0.3143 - val_loss: 1.3828 - val_acc: 0.2699\n",
      "Epoch 14/200\n",
      "18557/18557 [==============================] - 25s 1ms/step - loss: 1.3746 - acc: 0.3165 - val_loss: 1.3785 - val_acc: 0.2738\n",
      "Epoch 15/200\n",
      "18557/18557 [==============================] - 24s 1ms/step - loss: 1.3698 - acc: 0.3200 - val_loss: 1.3845 - val_acc: 0.2837\n",
      "Epoch 16/200\n",
      "18557/18557 [==============================] - 24s 1ms/step - loss: 1.3657 - acc: 0.3225 - val_loss: 1.3714 - val_acc: 0.2794\n",
      "Epoch 17/200\n",
      "18557/18557 [==============================] - 24s 1ms/step - loss: 1.3619 - acc: 0.3237 - val_loss: 1.3673 - val_acc: 0.2865\n",
      "Epoch 18/200\n",
      "18557/18557 [==============================] - 24s 1ms/step - loss: 1.3616 - acc: 0.3223 - val_loss: 1.3741 - val_acc: 0.2803\n",
      "Epoch 19/200\n",
      "18557/18557 [==============================] - 24s 1ms/step - loss: 1.3567 - acc: 0.3317 - val_loss: 1.3651 - val_acc: 0.2885\n",
      "Epoch 20/200\n",
      "18557/18557 [==============================] - 25s 1ms/step - loss: 1.3540 - acc: 0.3343 - val_loss: 1.3571 - val_acc: 0.2894\n",
      "Epoch 21/200\n",
      "18557/18557 [==============================] - 25s 1ms/step - loss: 1.3500 - acc: 0.3401 - val_loss: 1.3524 - val_acc: 0.3071\n",
      "Epoch 22/200\n",
      "18557/18557 [==============================] - 25s 1ms/step - loss: 1.3462 - acc: 0.3380 - val_loss: 1.3585 - val_acc: 0.3014\n",
      "Epoch 23/200\n",
      "18557/18557 [==============================] - 24s 1ms/step - loss: 1.3418 - acc: 0.3435 - val_loss: 1.3450 - val_acc: 0.3120\n",
      "Epoch 24/200\n",
      "18557/18557 [==============================] - 24s 1ms/step - loss: 1.3372 - acc: 0.3478 - val_loss: 1.3719 - val_acc: 0.2824\n",
      "Epoch 25/200\n",
      "18557/18557 [==============================] - 24s 1ms/step - loss: 1.3313 - acc: 0.3555 - val_loss: 1.3345 - val_acc: 0.3159\n",
      "Epoch 26/200\n",
      "18557/18557 [==============================] - 24s 1ms/step - loss: 1.3274 - acc: 0.3630 - val_loss: 1.3390 - val_acc: 0.3211\n",
      "Epoch 27/200\n",
      "18557/18557 [==============================] - 24s 1ms/step - loss: 1.3199 - acc: 0.3656 - val_loss: 1.3276 - val_acc: 0.3291\n",
      "Epoch 28/200\n",
      " 4864/18557 [======>.......................] - ETA: 17s - loss: 1.3180 - acc: 0.3690"
     ]
    }
   ],
   "source": [
    "RNN_stride = net.RNN(RNNconfig)\n",
    "\n",
    "RNN_stride.build_model(inception=True, res=True, maxpool=False, avgpool=False, batchnorm=True)\n",
    "\n",
    "m = RNN_stride.model\n",
    "# print(m.summary())\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "history_stride = RNN_stride.train(X_train, y_train, X_val, y_val, verbose=1)\n",
    "acc = RNN_stride.eval_acc(X_test, y_test)\n",
    "\n",
    "print(\"\\tTraining time: {}s\".format(time.time()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Plot History\n",
    "plt.plot(history_stride.history['acc'])\n",
    "plt.plot(history_stride.history['val_acc'])\n",
    "plt.title('model accuracy: stride of 2')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Loss\n",
    "plt.plot(history_stride.history['loss'])\n",
    "plt.plot(history_stride.history['val_loss'])\n",
    "plt.title('model loss: stride of 2')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Stats\n",
    "print(\"Max val Acc\", np.max(history_stride.history['val_acc']))\n",
    "print(\"Min val loss\", np.min(history_stride.history['val_loss']))\n",
    "print(\"Test Accuracy: {}\".format(acc))\n",
    "print(\"Max train Acc\", np.max(history_stride.history['acc']))\n",
    "\n",
    "# Save model\n",
    "RNN_max.save_model(\"model_stride.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_max = net.RNN(RNNconfig)\n",
    "\n",
    "RNN_max.build_model(inception=True, res=True, maxpool=True, avgpool=False, batchnorm=True)\n",
    "\n",
    "m = RNN_max.model\n",
    "# print(m.summary())\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "history_max = RNN_max.train(X_train, y_train, X_val, y_val, verbose=1)\n",
    "acc = RNN_max.eval_acc(X_test, y_test)\n",
    "\n",
    "print(\"\\tTraining time: {}s\".format(time.time()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot History\n",
    "plt.plot(history_max.history['acc'])\n",
    "plt.plot(history_max.history['val_acc'])\n",
    "plt.title('model accuracy: max pool')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Loss\n",
    "plt.plot(history_max.history['loss'])\n",
    "plt.plot(history_max.history['val_loss'])\n",
    "plt.title('model loss: max pool')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Stats\n",
    "print(\"Max val Acc\", np.max(history_max.history['val_acc']))\n",
    "print(\"Min val loss\", np.min(history_max.history['val_loss']))\n",
    "print(\"Test Accuracy: {}\".format(acc))\n",
    "print(\"Max train Acc\", np.max(history_max.history['acc']))\n",
    "\n",
    "# Save model\n",
    "RNN_max.save_model(\"model_max.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3: Average pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_avg = net.RNN(RNNconfig)\n",
    "\n",
    "RNN_avg.build_model(inception=True, res=True, maxpool=False, avgpool=True, batchnorm=True)\n",
    "\n",
    "m = RNN_avg.model\n",
    "# print(m.summary())\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "history_avg = RNN_avg.train(X_train, y_train, X_val, y_val, verbose=1)\n",
    "acc = RNN_avg.eval_acc(X_test, y_test)\n",
    "\n",
    "print(\"\\tTraining time: {}s\".format(time.time()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot History\n",
    "plt.plot(history_avg.history['acc'])\n",
    "plt.plot(history_avg.history['val_acc'])\n",
    "plt.title('model accuracy: average pool')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Loss\n",
    "plt.plot(history_avg.history['loss'])\n",
    "plt.plot(history_avg.history['val_loss'])\n",
    "plt.title('model loss: average pool')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Stats\n",
    "print(\"Max val Acc\", np.max(history_avg.history['val_acc']))\n",
    "print(\"Min val loss\", np.min(history_avg.history['val_loss']))\n",
    "print(\"Test Accuracy: {}\".format(acc))\n",
    "print(\"Max train Acc\", np.max(history_avg.history['acc']))\n",
    "\n",
    "# Save model\n",
    "RNN_max.save_model(\"model_avg.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
